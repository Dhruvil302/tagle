```text
â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—     â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—
â•šâ•â•â–ˆâ–ˆâ•”â•â•â•â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•”â•â•â•â•â• â–ˆâ–ˆâ•‘     â–ˆâ–ˆâ•”â•â•â•â•â•
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘     â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘     â–ˆâ–ˆâ•”â•â•â•  
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â•šâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—
   â•šâ•â•   â•šâ•â•  â•šâ•â• â•šâ•â•â•â•â•â• â•šâ•â•â•â•â•â•â•â•šâ•â•â•â•â•â•â•

                     T A G L E
        Your photos. Organized locally.
```
<p align="center">
  <img src="https://img.shields.io/badge/license-MIT-green.svg" />
  <img src="https://img.shields.io/badge/python-3.10%2B-blue" />
  <img src="https://img.shields.io/badge/offline-AI-orange" />
  <img src="https://img.shields.io/badge/captions-BLIP-black" />
  <img src="https://img.shields.io/badge/status-Phase%201-lightgrey" />
</p>


# ğŸ·ï¸ Tagle
> **Your photos, intelligently organized â€” all local.**

Tagle is a local-first photo intelligence tool.  
It scans your photos, extracts EXIF metadata, generates AI captions, builds searchable tags, and now (Phase 2) performs **semantic search** using CLIP embeddings + FAISS â€” all offline.
---

## ğŸš€ Features

- ğŸ–¼ï¸ **Local photo processing** â€” never uploads your photos  
- ğŸ§  **AI-generated captions** using BLIP (runs offline)  
- ğŸ·ï¸ **Automatic keyword tagging** (lightweight NLP)  
- ğŸ“… **EXIF extraction** â€” date, camera, GPS (when available)  
- ğŸ” **Fast keyword search** via CLI
- âœ¨ **CLIP semantic search (Phase 2)** via CLI
- âš¡ **FAISS vector index** for fast retrieval
- ğŸ’¾ **SQLite database** (simple, portable, scalable)  
- ğŸ” **Incremental updates** â€” only processes new files  

---

## ğŸ“¦ Project Structure
```markdown
tagle/
â”œâ”€â”€ data/                 # SQLite DB + FAISS index (ignored in git)
â”œâ”€â”€ photos/               # Your images go here
â”œâ”€â”€ cache/                # Model cache (ignored in git)
â”‚
â”œâ”€â”€ backend/                # Core source code
â”‚   â”œâ”€â”€ init_db.py        # Initialize the database
â”‚   â”œâ”€â”€ scan.py           # Scan folders & extract EXIF
â”‚   â”œâ”€â”€ caption.py        # Generate captions using BLIP
â”‚   â”œâ”€â”€ tagger.py         # Extract keyword tags
â”‚   â”œâ”€â”€ ingest.py         # Full pipeline runner
â”‚   â”œâ”€â”€ embedder.py       # Create Vector Embeddings 
â”‚   â”œâ”€â”€ build_faiss.py.   # Create Faiss Index
â”‚   â”œâ”€â”€ semantic_search.py# Search semantically
â”‚   â””â”€â”€ search_cli.py     # Simple keyword search
â”‚
â”œâ”€â”€ converted
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
---
```
## âš™ï¸ Installation

### 1. Create virtual environment
```bash
python -m venv .venv
source .venv/bin/activate      # Windows: .venv\Scripts\activate
```
2. Install dependencies
```bash
pip install "torch>=2.2" torchvision --extra-index-url https://download.pytorch.org/whl/cu121
pip install transformers==4.44.2 pillow piexif tqdm pandas nltk sqlite-utils
```
If youâ€™re on CPU-only, just run:
```bash
pip install torch torchvision
```
and PyTorch will install the CPU version automatically.

â¸»

Initialize the Database
```bash
python backend/init_db.py
```
This creates:

data/tagle.sqlite


â¸»

ğŸ“¸ Add Your Photos

Place any number of images into:

tagle/photos/

Supported formats:
	â€¢	JPG / JPEG
	â€¢	PNG
	â€¢	WEBP
	â€¢	TIFF
	â€¢	BMP
  â€¢	HEIC

â¸»

ğŸ§  Run Tagle (Full Pipeline)
```bash
python backend/ingest.py photos
```
This performs:
	1.	Folder scan
	2.	EXIF extraction
	3.	Caption generation (local BLIP model)
	4.	Keyword tagging
	5.	DB updates

You can run it anytime â€” it only processes new/unprocessed photos.

â¸»

ğŸ” Search Your Photos
```bash
python backend/search_cli.py beach
python backend/search_cli.py family sunset
python backend/search_cli.py dog 2018
```
Example output:

01. photos/beach_trip_2020.jpg
    date: 2020-06-18
    caption: A family walking on the beach during sunset.
    tags: family,walking,beach,sunset,trip


âœ¨ Phase 2 â€” Semantic Search (CLIP + FAISS)

2.1 Generate CLIP embeddings
```bash
python backend/embedder.py
```

2.2 Build FAISS index
```bash
python backend/build_faiss.py
```

You should now have:
```text
data/tagle.index
data/tagle_ids.npy
```

2.3 Run semantic search
```bash
python backend/semantic_search.py "dog on the beach"
```
or specify top K:
```bash
python backend/semantic_search.py "sunset mountains" 20
```

Example output:
```text
ğŸ“¸ photos/IMG_1123.jpg
    caption: A dog running along the beach at sunset.
    tags: dog,running,beach,sunset
```

â¸»

ğŸ§­ Design Philosophy

Principle
Local-first - No cloud APIs or uploads
Privacy - Photos never leave your machine
Offline AI - BLIP + CLIP models run locally
Modular - Each phase is independent
Scalable - Supports thousands+ photos
Extensible -Ready for UI, faces, filtering

â¸»

ğŸ§¾ License

MIT License Â© 2025 â€” Dhruvil Vasoya

â¸»

ğŸ’¬ Credits
	â€¢	Salesforce BLIP
	â€¢	OpenCLIP / SentenceTransformers
	â€¢	FAISS (Meta AI)
	â€¢	SQLite
	â€¢	Pillow
	â€¢	Community inspiration

â¸»

âœ¨ Why â€œTagleâ€?

Tagle = â€œTagâ€ + â€œGoogleâ€ (in spirit) â€”
A local AI memory assistant that helps you rediscover your photos anytime, without ever leaving your device.

â¸»

ğŸ”— Get Started
```bash
# First time only (if not done yet)
python backend/init_db.py

# Then, any time you add / change photos:
python backend/ingest.py photos

python backend/semantic_search.py "dog on the beach"
```
Tagle â€” your memories, retrieved with intelligence â€” and privacy.